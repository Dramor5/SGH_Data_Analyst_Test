---
title: "SGH Data Analyst test"
author: "Tan Wei Zhi"
date: "2023-10-20"
output: html_document
---

The objective of this assignment is to predict for diabetes (gh >= 6.5%). This will be performed using 3 machine learning models - logistic regression, decision tree and random forest. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# knitr::opts_knit$set(root.dir = 'C:/Users/Dramor/OneDrive - Nanyang Technological University/MIMIC-IV/mimic-iv-clinical-database-demo-2.2/icu')

# Define a relative path to a file or directory
relative_path <- file.path('Data', 'nhgh.tsv')
image_path <- file.path('Images','data_dictionary.png')


# Load packages
library(tidyverse)
library(ggplot2)
library(DBI)
library(naniar)
library(GGally)
library(reshape2)
library(data.table)
library(pheatmap)
library(vcd)
library(ggvenn)

# Setting global seed for reproducibility
set.seed(777)

```


![Data dictionary](`r image_path`)

There are two forms of diabetes - Type I and Type II. Type I is often due to genetic issues that causes the pancreas to stop producing insulin. Type II is more common worldwide, and is characterized by the downregulated production of insulin or cells in the body become unresponsive to insulin. Both lead to the inability of cells to uptake glucose, resulting in high glucose concentrations in the blood.

The exact mechanistic cause has Type II diabetes has yet to be pinned down. Despite this, it is associated with factors such as diet, obesity, age, genetics, and sex. By looking at these features of an individual, a machine-learning model may be constructed to predict if someone has Type II diabetes. 

The rationale behind the usage of features provided will be explained below:

  sex, age: Known to be associated with Type II diabetes, both will be used.
  
  re: Genetic factors differ greatly between individuals of different ethnicities. Although not a substitute for diabetes-associated genes, this feature should provide some information to distinguish affected and non-affected individuals.
  
  income: Income levels influence a person's diet and behavior. The income provided here is family income, which is not preferable as the individual in this dataset may not be the financial decision maker in the household. Alternatively, family income could be high, but money may not be distributed among the family unit. Hence the use of income in this analysis may become problematic, it will be analyzed, but it will have less priority as a feature in the machine learning model.
  
  tx, dx: The inclusion of these features in this dataset is interesting. The presence of medication or a diagnosis related to diabetes should be the strongest predictor for Type II diabetes. 
  
  wt, ht, bmi: bmi already takes into account both the weight and height features, but there could be a relationship between height and other anthropometric data below. bmi will be excluded while wt and ht will be used. 
  
  leg, arml: Studies were performed in the past to suggest that limb measurements could play a role in diabetes prediction. Limb lengths are indicators of an individual's diet during childhood and limb to body proportions could have effects on diabetes susceptibility. (https://www.cambridge.org/core/services/aop-cambridge-core/content/view/F48929D1213F5C8457CF8BF599F62047/S1368980022001215a.pdf/upper_arm_length_and_knee_height_are_associated_with_diabetes_in_the_middleaged_and_elderly_evidence_from_the_china_health_and_retirement_longitudinal_study.pdf)
  
  armc, waist, tri, sub: anthropometric data can be used to determine body shapes and proportions. There is literature on all of these features and their association with Type II diabetes. All will be used. 
  
  Some studies have reported that waist-to-height ratio is a strong predictor for diabetes. (https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-019-7801-2#:~:text=Anthropometric%20measures%20including%20waist%20circumference,risk%20of%20diabetes%20and%20hypertension.) Subscapular skinfold was also found to be associated with Type II diabetes in a Peruvian population. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6960014/)
  
  gh: Glycohaemoglobin is a clinical marker for blood glucose levels. This what we are predicting for, and will be transformed to a categorical variable such that gh >= 6.5% is 1 while gh < 6.5%  is 0.
  
  albumin, bun, SCr: These are indicators of kidney function and damage. Elevated levels of albumin, blood urea nitrogen and creatinine are associated with kidney damage, and kidney damage is associated with uncontrolled diabetes.




Despite being a small dataset, it will be transformed into a SQLite database to showcase SQL skills. I will try to do most manipulations in SQL to reduce the size of the dataframe (if necessary) before loading it into memory.

```{r eval = TRUE, echo=TRUE}
# Loading of data.
df <- read_tsv(relative_path,
               show_col_types = FALSE)

# Open a SQLite connection.
conn <- dbConnect(RSQLite::SQLite(), dbname = 'diabetes.db')

# dbRemoveTable(conn,'diabetes_data')
# dbRemoveTable(conn,'diabetes_data_modified')

# Create a new table within the database.
dbWriteTable(conn,
             name = 'diabetes_data',
             value = df,
             overwrite = TRUE)

```

```{r eval = TRUE, echo=TRUE}
# Analyze the whole dataset first.
query <- 'SELECT * FROM diabetes_data'
df <- dbGetQuery(conn, query)

na_plot <- vis_miss(df)
print(na_plot)
```
A visual representation of NAs is preferable to looking at NA counts in the data dictionary. The majority of NAs come from the sub column, at 14% while other anthropometric columns range from 3-7% missingness. income and kidney damage-related columns also contain small amounts of NA. One solution is to drop all rows that have atleast one NA in them. We first need to count the number of rows that fit this criteria, and the amount of data that we'll lose.

```{r eval = TRUE, echo=TRUE}
# Count the number of entries.
query <- "SELECT COUNT(*) FROM diabetes_data"

initial_row_count <- dbGetQuery(conn, query)

print(paste('Initial row count:', initial_row_count))

# Count number of entries with atleast one NA in them.
query <- "SELECT COUNT(*) FROM diabetes_data WHERE income iS NULL OR leg IS NULL OR arml IS NULL OR armc IS NULL OR waist IS NULL OR tri IS NULL OR sub IS NULL OR albumin IS NULL OR bun IS NULL OR Scr IS NULL"

row_count_no_NA <- dbGetQuery(conn, query)

print(paste('Rows with atleast one NA:', row_count_no_NA))

# Calculate the amount of data lost.
print(paste('Percent data lost: ', round(row_count_no_NA/initial_row_count*100), '%', sep = ''))
```
21% of data will be lost if we were to take this approach. But after removal there would still be >5000 unique rows of patient data available. The amount of data required is specific to each problem and more complex problems or algorithms used would require more data. 
My opinion is that >5000 rows of data is still sufficient to build a model for diabetes prediction, and the current dataset satisfies the rule of thumb where the number of data points should be at least 10x the number of features. 

However, to further showcase my skills I will perform imputation using KNN. The idea is that NAs will be replaced with the mean value from the n nearest neighbors found in the training dataset. More exploratory data analysis will be done, and the results of imputation will be compared with the original exploratory data analysis.

```{r eerfqwf, echo=TRUE}
# Obtain continuous variables.
query <- 'SELECT age, wt, ht, bmi, leg, arml, armc, waist, tri, sub, gh, albumin, bun, SCr FROM diabetes_data'
continuous_df <- dbGetQuery(conn, query)

# Transform into a two col dataframe of variables and values. This is necessary for facet_wrap().
continuous_df_long <- melt(continuous_df)

# For each variable, plot a violin+boxplot.
continuous_plot <- ggplot(continuous_df_long, aes(x = variable, y = value)) + 
  geom_violin(width = 1) +
  geom_boxplot(width = 0.5) +
  facet_wrap(~ variable, scales = "free") +
  theme_classic()

continuous_plot
summary(continuous_df)

```
From the boxplots and summary table, there appears to be individuals aged below 20 in the dataset. Although more younger people are developing diabetes, people usually develop diabetes when they hit the age of 45 (https://www.cdc.gov/diabetes/basics/type2.html). Therefore, there would not be much of a point to include people who are too young in the dataset as it might introduce more noise. 

An interesting observation is the subset of morbidly obese individuals in the dataset. This is described by the wt and bmi features, along with features describing body shape such as waist and armc. These individuals have measurements beyond the upper whiskers and would potentially be outliers, but after some googling these measurements are still within the realm of possibility. I suspect these individuals are also the ones with the abnormally high gh value, but more analysis will have to take place before that conclusion can be drawn.

This is the phase where I would determine if outliers are within reason. Human errors made during data entry can sometimes form outliers so a reality check, and sometimes consulting a clinician may be necessary. But apart from the presence of young individuals and the morbidly obese subset, there appears to be nothing strange here. 

The warning shown in the output is due to the removal of a total of 2556 NA values, which cannot be plotted. 2556 is the total number of NA values here, and not the total rows containing atleast one NA value.  

```{r}
# Obtain categorical variables and gh.
query <- 'SELECT sex, re, income, tx, dx, gh FROM diabetes_data'
categorical_df <- dbGetQuery(conn, query)

# Transform gh to categorical variable.
categorical_df <- categorical_df %>%
  mutate(gh = case_when(gh >= 6.5 ~ 1,
                        T ~ c(0)))

# Create bar plots 
for (i in colnames(categorical_df)) {
  if (i == 'income') {
    p <- ggplot(categorical_df) +
    geom_bar(aes_string(x = i)) +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  print(p)
  } 
  else {
  p <- ggplot(categorical_df) +
    geom_bar(aes_string(x = i)) +
    theme_classic()
  print(p)
  }
}

# Transform into long format, for all categories, count all values observed.
altered_pie_df <- categorical_df %>%
  gather(key = 'category') %>%
  count(category, value) %>%
  mutate(perc_n = round(n/nrow(categorical_df)*100, 2))

# Create pie charts for every categorical variable.
for (i in unique(altered_pie_df$category)) {
  p <- altered_pie_df[altered_pie_df$category == i, ] %>%
  ggplot(aes(x="", y=n, fill=value)) +
    geom_col(color = 'black') + 
    geom_text(aes(label = paste(perc_n, '%')),
            position = position_stack(vjust = 0.5)) +
    coord_polar("y", start=0) +
    theme_void() +
    ggtitle(paste(i)) + 
    theme(plot.title = element_text(hjust = 0.5))
    
  print(p)
}
```
The bar plots visually depict count values for comparison, but the pie charts are more useful when it comes to looking at proportion. Unfortunately the pie chart for income has too many categories, causing labels to overlap. 

Interestingly, the number of diagnoses outnumber the number of people with diabetes (dx). This is probably due to the presence of pre-DM diagnoses in the dx feature. Pre-diabetes has a lower gh threshold and should explain the mismatch diabetic individuals (based on gh) and individuals in dx. 

Out of curiosity I checked the number of pre-diabetics within the dataset. 

```{r}
# Prepare dataframe for plotting venn diagram.
pDM_or_DM_venn <- categorical_df %>%
  select(dx, gh) %>%
  
  # Filter for row sums >= 1 to exclude individuals who are not in either category. (Not diabetic, not preDM)
  filter(rowSums(.) >= 1) %>%
  
  # Transform into T/F for venn diagram plotting.
  mutate(dx = as.logical(dx),
         gh = as.logical(gh)) %>%
  rename('preDM or DM' = dx,
         'DM' = gh)


# Plot the venn diagram.
ggvenn(pDM_or_DM_venn, c('preDM or DM', 'DM'))


# The individual's diabetes status is more clearly illustrated below.
 
# dx = 0, gh = 0 -> no preDM or DM
# dx = 1, gh = 1 -> DM
# dx = 1, gh = 0 -> preDM
# dx = 0, gh = 1 -> DM

```

About half of individuals marked with preDM or DM through the dx feature is preDM. There is a tradeoff in using dx as a feature for the model. I believe it would give the model a higher sensitivity score, where it is more adept at picking out diabetics from the overall population, but its precision score may falter as some false-positives will be picked out.

At this point I'm unsure if the other features are as valuable, we would have to look for correlations with diabetes to determine this and we need to check if features are correlation to other features as well.

```{r}
# Load data.
query <- 'SELECT age, wt, ht, bmi, leg, arml, armc, waist, tri, sub, gh, albumin, bun, SCr FROM diabetes_data'
continuous_df <- dbGetQuery(conn, query)

# Construct paired geom_point graphs with different features. 
continuous_df <- continuous_df %>%
  na.omit() %>%
  mutate(diabetes = case_when(gh >= 6.5 ~ 1,
                              T ~ 0))

pair_graphs <- ggpairs(continuous_df, aes(col = diabetes),
        progress = F, 
        lower = list(continuous = "points"),
        upper = list(continuous = "blank"),
        diag = list(continuous = "blankDiag"),
        axisLabels = 'none')

pair_graphs

```
The light blue refers to diabetic patients, while dark blue refers to non-diabetic patients. Unfortunately, there is no discernable relationship between gh and any of the variables. In the age vs gh graph, it appears the threshold for diabetes has somehow dropped, but this is likely due to overplotting. In all other graphs with age as the x-axis, the diabetic population seems to form towards the right of the graph. A possible method to make the dataset balanced would be to filter for older individuals. 

Another method of visualizing correlations is through construction of a correlation matrix, and plotting the results through a heatmap.

```{r}
# Produce a correlation matrix from a dataframe (pearson's correlation coefficient)
cont_heatmap <- cor(continuous_df)

# Plot the heatmap
pheatmap(cont_heatmap,
         display_numbers = T)

```
There were positive relationships between anthropometric variables such as (ht and leg), (wt, bmi, waist, armc and sub). It would be wise to select a feature that would represent the values that are correlated to one another, as they bring the same information to the model. In terms of correlation with the gh variable, only age, waist, bmi and sub had weak notable Pearson's correlation coefficients of >0.2. Further, albumin had a weakly negative correlation with diabetes, with a value of -0.21. 

We take a closer look at the different continuous features and plot them against gh to examine their distributions.

```{r}
# Plot all continous variables against gh.
for (i in colnames(continuous_df)) {
  p <- ggplot(continuous_df) +
    geom_point(aes_string(x = i, y = 'gh', col = 'diabetes')) + 
    theme_classic()
  print(p)
}
```

Taking a closer look at the graph of features against gh reveals that age, bmi, armc, waist may be good predictors of diabetes, since the diabetics are always concentrated along a certain region of the x-axis. Apart from looking purely at graphs we can do a t-test between the means of different features, in diabetics and non-diabetics to check for significance. This will be done with a two-sample test with significance level at p = 0.05.

```{r}
# Initialize a new list for result collection.
t_test_df <- list()

# For all continuous variables except the diabetes columns, do a t-test of means between diabetics and non-diabetics. 
for (i in colnames(continuous_df)[1:length(colnames(continuous_df)) - 1]) {
  
  # Perform the t-test.
  t_res <- t.test(continuous_df[continuous_df$diabetes == 1, colnames(continuous_df) == i],
                  continuous_df[continuous_df$diabetes == 0, colnames(continuous_df) == i])
  
  # Collect results into a list.
  entry <- list('col_name' = i,
                'p_value' = t_res$p.value,
                'diabetic_mean' = round(t_res$estimate[[1]],2),
                'non_diabetic_mean' = round(t_res$estimate[[2]], 2))
              
  # Add the entry into a list.  
  t_test_df[[i]] <- entry
}

# R-bind all lists to form a dataframe.
t_test_df <- rbindlist(t_test_df) %>%
  arrange(p_value) %>%
  mutate(significant = case_when(p_value < 0.05 ~ T,
                                 T ~ F))

t_test_df

```
The t-test shows p-values in comparisons of feature means between diabetics and non-diabetics. If we were to select for features, we would pick from the top, with the lowest p-value and move downwards.

I will do the same with categorical variables, using the chi-squared test of independence. In addition, Cramer's V will be calculated to measure the strength of the association.

```{r}
# Obtain categorical variables and gh.
query <- 'SELECT sex, re, income, tx, dx, gh FROM diabetes_data'
categorical_df <- dbGetQuery(conn, query) %>%
  mutate(diabetes = case_when(gh >= 6.5 ~ T,
                              T ~ F)) %>%
  select(-gh)

# Initialize empty list.
chi_sq_df <- list()

# For all columns except the diabetes marker, perform chi square test. 
# Determine if variable in question is significantly associated with diabetes.
for (i in colnames(categorical_df)[1:length(colnames(categorical_df)) - 1]) {
  
  # Construct contingency table, perform chi square test. 
  cont_table <- table(categorical_df[[i]], categorical_df$diabetes)
  
  chi_squared_result <- chisq.test(cont_table)
  
  cramers_v_result <- assocstats(cont_table)
  
  # Store result in a list.
  entry <- list('col_name' = i,
                'chi-sq_stat' = chi_squared_result$statistic,
                'p_value' = chi_squared_result$p.value,
                'cramers_v' = cramers_v_result$cramer)
  
  chi_sq_df[[i]] <- entry
}

# R-bind all lists to form a dataframe.
chi_sq_df <- rbindlist(chi_sq_df) %>%
  arrange(p_value) %>%
  mutate(significant = case_when(p_value < 0.05 ~ T,
                                 T ~ F))

chi_sq_df
```
All categorical variables had a significant association with diabetes. tx and dx had a Cramer's V value of 0.65 and 0.58 respectively, indicating that they were more strongly associated with diabetes compared to income, re and sex. 

Based on the observations gathered, feature selection will take place. All features in this dataset contribute to diabetes in some way. The best method of performing feature selection would probably be to rely on domain knowledge to filter for features (has already been done for this dataset, given that all features contribute to diabetes), before using wrapper methods to search for the best subset of features to use. Although reading through all the literature on diabetes ML models would enable me to make a better model, due to time scarcity, lack of computational power and inexperience with wrapper methods, I will select features based on information I gathered from this dataset. 

The features: age, bmi, albumin, tx and dx will be used. Justification:

  - Age has the highest Pearson's correlation coefficient value in contrast to all other features (heatmap). Diabetics were found to be concentrated past a certain age threshold (Age vs gh scatterplot). There difference between mean ages of diabetics and non-diabetics was 20 years, and was statistically significant (t-test). 
  
  - BMI has the 3rd highest Pearson's correlation coefficient value, behind waist (heatmap). BMI is used in favor of waist as it also describes height, while being highly correlated to waist as well (PCC = 0.9), making it a descriptor of body proportions. While not as extreme as age, the frequency of diabetics increases above a certain BMI threshold (bmi vs gh scatterplot). The difference in mean BMI between diabetics and non-diabetics is 4, this difference was also statistically significant (t-test). 

  - Albumin is negatively correlated with gh, with a Pearson's correlation coefficient of -0.15 (heatmap). This negative relationship is not as strong as that of Age or BMI, but this feature offers information with regards to kidney function, which is not captured in other features. There was a significant difference in albumin levels between diabetics and non-diabetics (t-test), but whether this difference is biologically significant or not is a question for a physician. Nevertheless, I will use this feature in hopes of incorporating kidney function into the model.
  
  - tx and dx proportions closely mimicked that of diabetic individuals (pie charts). These features were associated with diabetics, and had higher Cramer's V values compared to other categorical variables. The presence of insulin/diabetes medicine (tx) should be a tell-tale sign of diabetes. For dx, while about half of them marked as preDM or DM are preDM, I believe it is better for the precision score of the model to suffer in favor of a higher sensitivity score to maximally capture all incidents of DM in the community. The false-positives incurred will likely be from the preDM population as well so there is minimal harm here. (Clinicians may disagree with me, depending on the treatment differences of preDM and DM.)



```{r eval=FALSE, include=FALSE}
# Create a copy of the table for my own modifications.
query <- "CREATE TABLE diabetes_data_modified AS SELECT * FROM diabetes_data"
dbExecute(conn, query)

# Remove BMI column in copied table.
query <- "ALTER TABLE diabetes_data_modified DROP COLUMN bmi;"
dbExecute(conn, query)


### Check cols
# query <- "PRAGMA table_info(diabetes_data_modified);"
# column_info <- dbGetQuery(conn, query)
# print(column_info)
###



# List all tables in the database
tables <- dbListTables(conn)

# Print the list of tables
cat("Tables in the database:\n")
cat(tables, sep = "\n")

```





