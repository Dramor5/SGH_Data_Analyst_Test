---
title: "SGH Data Analyst test"
author: "Tan Wei Zhi"
date: "2023-10-20"
output: html_document
---

The objective of this assignment is to predict for diabetes (gh >= 6.5%). This will be performed using 3 machine learning models - logistic regression, decision tree and random forest. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Define a relative path to a file or directory
relative_path <- file.path('Data', 'nhgh.tsv')
image_path <- file.path('Images','data_dictionary.png')

# Load packages
library(tidyverse)
library(ggplot2)
library(DBI)
library(naniar)
library(GGally)
library(reshape2)
library(data.table)
library(pheatmap)
library(vcd)
library(ggvenn)
library(VIM)
library(gridExtra)
library(caTools)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gt)

# Setting global seed for reproducibility
set.seed(777)

```


![Data dictionary](`r image_path`)


# **Introduction**

There are two forms of diabetes - Type I and Type II. Type I is often due to genetic issues that causes the pancreas to stop producing insulin. Type II is more common worldwide, and is characterized by the downregulated production of insulin or cells in the body become unresponsive to insulin. Both lead to the inability of cells to uptake glucose, resulting in high glucose concentrations in the blood.

The exact mechanistic cause has Type II diabetes has yet to be pinned down. Despite this, it is associated with factors such as diet, obesity, age, genetics, and sex. By looking at these features of an individual, a machine-learning model may be constructed to predict if someone has Type II diabetes. 

The relationship between the given features and diabetes is shown below:

  sex, age: Known to be associated with Type II diabetes.
  
  re: Genetic factors differ greatly between individuals of different ethnicities. Although not a substitute for diabetes-associated genes, this feature should provide some information to distinguish affected and non-affected individuals.
  
  income: Income levels influence a person's diet and behavior. The income provided here is family income, which is not preferable as the individual in this dataset may not be the financial decision maker in the household. Alternatively, family income could be high, but money may not be distributed among the family unit. Hence the use of income in this analysis may become problematic, it will be analyzed, but it will have less priority as a feature in the machine learning model.
  
  tx, dx: The inclusion of these features in this dataset is interesting. The presence of medication or a diagnosis related to diabetes should be the strongest predictor for Type II diabetes. 
  
  wt, ht, bmi: bmi already takes into account both the weight and height features, but there could be a relationship between height and other anthropometric data below.
  
  leg, arml: Studies were performed in the past to suggest that limb measurements could play a role in diabetes prediction. Limb lengths are indicators of an individual's diet during childhood and limb to body proportions could have effects on diabetes susceptibility.
  
  armc, waist, tri, sub: Anthropometric data can be used to determine body shapes and proportions. There is literature on all of these features and their association with Type II diabetes. 
  
  gh: Glycohaemoglobin is a clinical marker for blood glucose levels. This what we are predicting for, and will be transformed to a categorical variable such that gh >= 6.5% is 1 while gh < 6.5%  is 0.
  
  albumin, bun, SCr: These are indicators of kidney function and damage. Elevated levels of albumin, blood urea nitrogen and creatinine are associated with kidney damage, and kidney damage is associated with uncontrolled diabetes.

# **Loading of data**

Despite being a small dataset, it will be transformed into a SQLite database to showcase SQL skills. I will try to do most manipulations in SQL to reduce the size of the dataframe (if necessary) before loading it into memory. The dataset was checked for duplications before doing so.

```{r}
# Loading of data.
df <- read_tsv(relative_path,
               show_col_types = FALSE)

# Check for duplications in seqn.
sum(duplicated(df$seqn))

# Open a SQLite connection.
conn <- dbConnect(RSQLite::SQLite(), dbname = 'diabetes.db')

# dbRemoveTable(conn,'diabetes_data')
# dbRemoveTable(conn,'diabetes_data_modified')

# Create a new table within the database.
dbWriteTable(conn,
             name = 'diabetes_data',
             value = df,
             overwrite = TRUE)

```
There were no seqn duplications, indicating that all individuals in the dataset were unique. Therefore the dataset is already in a tidy format (one observation = one row). It was subsequently loaded into a new table within the SQLite connection.

# **Exploratory data analysis**

```{r}

# Analyze the whole dataset first.
query <- 'SELECT * FROM diabetes_data'
df <- dbGetQuery(conn, query)

# Produce an NA plot.
na_plot <- vis_miss(df)
print(na_plot)

# Look at unique combinations of missing data.
aggr(df)

```

A visual representation of NAs is preferable to looking at NA counts in the data dictionary. The majority of NAs come from the sub column, at 14% while other anthropometric columns range from 3-7% missingness. income and kidney damage-related columns also contain small amounts of NA. One solution is to drop all rows that have atleast one NA in them. We first need to count the number of rows that fit this criteria, and the amount of data that we'll lose.

```{r}
# Count the number of entries.
query <- "SELECT COUNT(*) FROM diabetes_data"

initial_row_count <- dbGetQuery(conn, query)

print(paste('Initial row count:', initial_row_count))

# Count number of entries with atleast one NA in them.
query <- "SELECT COUNT(*) FROM diabetes_data WHERE income iS NULL OR leg IS NULL OR arml IS NULL OR armc IS NULL OR waist IS NULL OR tri IS NULL OR sub IS NULL OR albumin IS NULL OR bun IS NULL OR Scr IS NULL"

row_count_no_NA <- dbGetQuery(conn, query)

print(paste('Rows with atleast one NA:', row_count_no_NA))

# Calculate the amount of data lost.
print(paste('Percent data lost: ', round(row_count_no_NA/initial_row_count*100), '%', sep = ''))
```
21% of data will be lost if we were to take this approach. But after removal there would still be >5000 unique rows of patient data available. The amount of data required is specific to each problem and more complex problems or algorithms used would require more data. 
My opinion is that >5000 rows of data is still sufficient to build a model for diabetes prediction, and the current dataset satisfies the rule of thumb where the number of data points should be at least 10x the number of features. 

However, to further showcase my skills I will perform imputation using KNN. The idea is that NAs will be replaced with the mean value from a specified number of nearest neighbors found in the training dataset. More exploratory data analysis will be done before this occurs.

```{r}

# Obtain continuous variables.
query <- 'SELECT age, wt, ht, bmi, leg, arml, armc, waist, tri, sub, gh, albumin, bun, SCr FROM diabetes_data'
continuous_df <- dbGetQuery(conn, query)

# Transform into a two col dataframe of variables and values. This is necessary for facet_wrap().
continuous_df_long <- melt(continuous_df)

# For each variable, plot a violin+boxplot.
continuous_plot <- ggplot(continuous_df_long, aes(x = variable, y = value)) + 
  geom_violin(width = 1) +
  geom_boxplot(width = 0.5) +
  facet_wrap(~ variable, scales = "free") +
  theme_classic()

continuous_plot
summary(continuous_df)

```
From the boxplots and summary table, there appears to be individuals aged below 20 in the dataset. Although more younger people are developing diabetes, people usually develop diabetes when they hit the age of 45 (https://www.cdc.gov/diabetes/basics/type2.html). Therefore, there would not be much of a point to include people who are too young in the dataset as it might introduce more noise. 

An interesting observation is the subset of morbidly obese individuals in the dataset. This is described by the wt and bmi features, along with features describing body shape such as waist and armc. These individuals have measurements beyond the upper whiskers and would potentially be outliers, but after some googling these measurements are still within the realm of possibility.

This is the phase where I would determine if outliers are within reason. Human errors made during data entry can sometimes form outliers so a reality check, and sometimes consulting a clinician may be necessary. But apart from the presence of young individuals and the morbidly obese subset, there appears to be nothing strange here. 

The warning shown in the output is due to the removal of a total of 2556 NA values, which cannot be plotted. 2556 is the total number of NA values here, and not the total rows containing atleast one NA value.  

```{r}
# Obtain categorical variables and gh.
query <- 'SELECT sex, re, income, tx, dx, gh FROM diabetes_data'
categorical_df <- dbGetQuery(conn, query)

# Transform gh to categorical variable.
categorical_df <- categorical_df %>%
  mutate(gh = case_when(gh >= 6.5 ~ 1,
                        T ~ c(0)))

# Create bar plots.
for (i in colnames(categorical_df)) {
  if (i == 'income') {
    p <- ggplot(categorical_df) +
    geom_bar(aes_string(x = i)) +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
  print(p)
  } 
  else {
  p <- ggplot(categorical_df) +
    geom_bar(aes_string(x = i)) +
    theme_classic()
  print(p)
  }
}

# Transform into long format, for all categories, count all values observed.
altered_pie_df <- categorical_df %>%
  gather(key = 'category') %>%
  count(category, value) %>%
  mutate(perc_n = round(n/nrow(categorical_df)*100, 2))

# Create pie charts for every categorical variable.
for (i in unique(altered_pie_df$category)) {
  p <- altered_pie_df[altered_pie_df$category == i, ] %>%
  ggplot(aes(x="", y=n, fill=value)) +
    geom_col(color = 'black') + 
    geom_text(aes(label = paste(perc_n, '%')),
            position = position_stack(vjust = 0.5)) +
    coord_polar("y", start=0) +
    theme_void() +
    ggtitle(paste(i)) + 
    theme(plot.title = element_text(hjust = 0.5))
    
  print(p)
}
```

The bar plots depict count values for comparison, but the pie charts are more useful when it comes to looking at proportion. Unfortunately the pie chart for income has too many categories, causing labels to overlap. 

Interestingly, the number of diagnoses outnumber the number of people with diabetes (dx). This is probably due to the presence of pre-DM diagnoses in the dx feature. Pre-diabetes has a lower gh threshold and should explain the mismatch diabetic individuals (based on gh) and individuals in dx. 

Out of curiosity I checked the number of pre-diabetics within the dataset. 

```{r}
# Prepare dataframe for plotting venn diagram.
pDM_or_DM_venn <- categorical_df %>%
  select(dx, gh) %>%
  
  # Filter for row sums >= 1 to exclude individuals who are not in either category. (Not diabetic, not preDM)
  filter(rowSums(.) >= 1) %>%
  
  # Transform into T/F for venn diagram plotting.
  mutate(dx = as.logical(dx),
         gh = as.logical(gh)) %>%
  rename('preDM or DM' = dx,
         'DM' = gh)

# Plot the venn diagram.
ggvenn(pDM_or_DM_venn, c('preDM or DM', 'DM'))

# The individual's diabetes status is more clearly illustrated below.
 
# dx = 0, gh = 0 -> no preDM or DM
# dx = 1, gh = 1 -> DM
# dx = 1, gh = 0 -> preDM
# dx = 0, gh = 1 -> DM

```

About half of individuals marked with preDM or DM through the dx feature is preDM. There is a tradeoff in using dx as a feature for the model. I believe it would give the model a higher sensitivity score, where it is more adept at picking out diabetics from the overall population, but its precision score may falter as some false-positives will be picked out.

At this point I'm unsure if the other features are as valuable, we would have to look for correlations with diabetes to determine this and we need to check if features are correlation to other features as well.

```{r}
# Load data with only continuous variables.
query <- 'SELECT age, wt, ht, bmi, leg, arml, armc, waist, tri, sub, gh, albumin, bun, SCr FROM diabetes_data'
continuous_df <- dbGetQuery(conn, query) 

# Remove NAs, add a categorical variable `diabetes` containing diabetic labels.
continuous_df <- continuous_df %>%
  na.omit() %>%
  mutate(diabetes = case_when(gh >= 6.5 ~ 1,
                              T ~ 0))

```

A method of visualizing correlations is through construction of a correlation matrix, and plotting the results through a heatmap.

```{r}
# Produce a correlation matrix from a dataframe (Pearson's correlation coefficient).
cont_heatmap <- cor(continuous_df)

pheatmap(cont_heatmap,
         display_numbers = T)

```

There were positive relationships between anthropometric variables such as (ht and leg), (wt, bmi, waist, armc and sub). It would be wise to select a feature that would represent the values that are correlated to one another, as they bring the same information to the model. In terms of correlation with the gh variable, only age, waist, bmi and sub had weak notable Pearson's correlation coefficients of >0.2. Further, albumin had a weakly negative correlation with diabetes, with a value of -0.21. 

We take a closer look at the different continuous features and plot them against gh to examine their distributions.

```{r}
# Plot all continuous variables against gh.
for (i in colnames(continuous_df)) {
  p <- ggplot(continuous_df) +
    geom_point(aes_string(x = i, y = 'gh', col = 'diabetes')) + 
    theme_classic()
  print(p)
}
```

The graph of features against gh reveals that age, bmi, armc, waist may be good predictors of diabetes, since the diabetics are always concentrated along a certain region of the x-axis. Apart from looking purely at graphs we can do a t-test between the means of different features, in diabetics and non-diabetics to check for significance. This will be done with a two-sample test with significance level at p = 0.05.

```{r}
# Initialize a new list for result collection.
t_test_df <- list()

# For all continuous variables except the diabetes columns, do a t-test of means between diabetics and non-diabetics. 
for (i in colnames(continuous_df)[1:length(colnames(continuous_df)) - 1]) {
  
  # Perform the t-test.
  t_res <- t.test(continuous_df[continuous_df$diabetes == 1, colnames(continuous_df) == i],
                  continuous_df[continuous_df$diabetes == 0, colnames(continuous_df) == i])
  
  # Collect results into a list.
  entry <- list('col_name' = i,
                'p_value' = t_res$p.value,
                'diabetic_mean' = round(t_res$estimate[[1]],2),
                'non_diabetic_mean' = round(t_res$estimate[[2]], 2))
              
  # Add the entry into a list.  
  t_test_df[[i]] <- entry
}

# R-bind all lists to form a dataframe.
t_test_df <- rbindlist(t_test_df) %>%
  arrange(p_value) %>%
  mutate(significant = case_when(p_value < 0.05 ~ T,
                                 T ~ F))

t_test_df

```
The t-test shows p-values in comparisons of feature means between diabetics and non-diabetics. There were significant differences between the all feature means of diabetics and non-diabetics. This can be combined with the results of the heatmap for feature selection.

The same will be done for the categorical features, using the chi-squared test of independence. In addition, Cramer's V will be calculated to measure the strength of the association.

```{r}
# Obtain categorical variables and gh.
query <- 'SELECT sex, re, income, tx, dx, gh FROM diabetes_data'
categorical_df <- dbGetQuery(conn, query) %>%
  mutate(diabetes = case_when(gh >= 6.5 ~ T,
                              T ~ F)) %>%
  select(-gh)

# Initialize empty list.
chi_sq_df <- list()

# For all columns except the diabetes marker, perform chi square test. 
# Determine if variable in question is significantly associated with diabetes.
for (i in colnames(categorical_df)[1:length(colnames(categorical_df)) - 1]) {
  
  # Construct contingency table, perform chi square test. 
  cont_table <- table(categorical_df[[i]], categorical_df$diabetes)
  
  chi_squared_result <- chisq.test(cont_table)
  
  cramers_v_result <- assocstats(cont_table)
  
  # Store result in a list.
  entry <- list('col_name' = i,
                'chi-sq_stat' = chi_squared_result$statistic,
                'p_value' = chi_squared_result$p.value,
                'cramers_v' = cramers_v_result$cramer)
  
  chi_sq_df[[i]] <- entry
}

# R-bind all lists to form a dataframe.
chi_sq_df <- rbindlist(chi_sq_df) %>%
  arrange(p_value) %>%
  mutate(significant = case_when(p_value < 0.05 ~ T,
                                 T ~ F))

chi_sq_df
```
All categorical variables had a significant association with diabetes. tx and dx had a Cramer's V value of 0.65 and 0.58 respectively, indicating that they were more strongly associated with diabetes compared to income, re and sex. 

# **Feature selection**

Based on the observations gathered, feature selection will take place. All features in this dataset contribute to diabetes in some way. The best method of performing feature selection would probably be to rely on domain knowledge to filter for features (has already been done for this dataset, given that all features contribute to diabetes), before using wrapper methods to search for the best subset of features to use. In addition, reading through all the literature on diabetes ML models would enable me to make a better model, but due to time scarcity, lack of computational power and inexperience with wrapper methods, I will select features based on information I gathered from this dataset. 

The features: age, bmi, albumin, tx and dx will be used. Justification:

  - Age has the highest Pearson's correlation coefficient value in contrast to all other features (heatmap). Diabetics were found to be concentrated past a certain age threshold (Age vs gh scatterplot). The difference between mean ages of diabetics and non-diabetics was 20 years, and was statistically significant (t-test). 
  
  - BMI has the 3rd highest Pearson's correlation coefficient value, behind waist (heatmap). BMI is used in favor of waist as it also describes height, while being highly correlated to waist as well (PCC = 0.9), making it a descriptor of body proportions. While not as extreme as age, the frequency of diabetics increases above a certain BMI threshold (bmi vs gh scatterplot). The difference in mean BMI between diabetics and non-diabetics is 4, this difference was also statistically significant (t-test). 

  - Albumin is negatively correlated with gh, with a Pearson's correlation coefficient of -0.15 (heatmap). This negative relationship is not as strong as that of Age or BMI, but this feature offers information with regards to kidney function, which is not captured in other features. There was a significant difference in albumin levels between diabetics and non-diabetics (t-test), but whether this difference is biologically significant or not is a question for a physician. Nevertheless, I will use this feature in hopes of incorporating kidney function into the model.
  
  - tx and dx proportions closely mimicked that of diabetic individuals (pie charts). These features were associated with diabetics, and had higher Cramer's V values compared to other categorical variables (Chi-squared test of independence). The presence of insulin/diabetes medicine (tx) should be a tell-tale sign of diabetes. For dx, while about half of them marked as preDM or DM are preDM, I believe it is better for the precision score of the model to suffer in favor of a higher sensitivity score to maximally capture all incidents of DM in the community. The false-positives incurred will likely be from the preDM population as well so there is minimal harm here. (Clinicians may disagree with me, depending on the treatment differences of preDM and DM.)
  
# **KNN imputation**

KNN imputation is next. The whole dataset will be used for KNN imputation so as to be more accurate, before selecting the relevant columns used for model building. KNN imputation is used, with Gower's distance as the distance metric and k = 5. Finally, individuals that are below the age of 20 will be removed, as the diabetes incidence is too low in this subset.

```{r}
# Load data.
query <- 'SELECT * FROM diabetes_data'
df <- dbGetQuery(conn, query)
dbDisconnect(conn) # SQL is no longer used after this. 

# Perform KNN imputation.
after_impute_df <- df %>%
  kNN(variable = 'albumin', 
      k = 5) %>%
  rename('albumin_imputed' = albumin_imp)

# Plot new distribution.
alb_after_plot <- ggplot(after_impute_df) +
  geom_point(aes(x = albumin, y = gh, col = albumin_imputed))

alb_after_plot

print(paste('albumin NA values before imputation:', sum(is.na(df$albumin))))

print(paste('albumin NA values after imputation:', sum(is.na(after_impute_df$albumin))))

summary(df$albumin)

summary(after_impute_df$albumin)

after_impute_df <- after_impute_df %>%
  select(age, bmi, albumin, tx, dx, gh) %>%
  mutate(diabetes = case_when(gh >= 6.5 ~ 1,
                              T ~ 0),
         diabetes = as.factor(diabetes)) %>%
  select(-gh)

# Count number of rows after imputation.
print(paste('Total number of rows:', nrow(after_impute_df)))

# Filter out individuals below the age of 20.
after_impute_df_filtered <- after_impute_df %>%
  filter(age > 20)

# Count number of rows after filtration.
print(paste('Total number of rows:', nrow(after_impute_df_filtered)))


```
A total of 89 values for albumin were imputed. Minimal differences could be seen in the albumin vector before and after imputation. Only the mean had decreased by 0.002. After imputation, the features age, bmi, albumin, tx and dx were used for modelling. There are a total of 5630 rows remaining after removing patients below the age of 20.


# **Model construction**

# Construction of logistic regression model

Logistic regression is a model used in classifying categorical variables like yes or no, 0 and 1. Linear regression is not suitable for this as the predicted value could be above 1 or below 0. In logistic regression, a sigmoidal curve is used to model the relationship between predictor variables and the probability of an event occurring. A logit function is used to transform the output to a value from 0 to 1, values >=0.5 will be classified as 1 while values 0.5< will be classified as 0. Both categorical and continuous features can be used for prediction. 

```{r}
# Split dataset in a 70:30 train:test ratio.
split <- sample.split(after_impute_df_filtered, SplitRatio = 0.7)

train_df <- subset(after_impute_df_filtered, split == T)
test_df <- subset(after_impute_df_filtered, split == F)
```


```{r}
# Construct control variable for log regression model, for k-fold cross validation.
log_control <- trainControl(method = 'cv', number = 10)

# Build a logistic regression model.
log_model <- train(diabetes ~ ., data = train_df, method = "glm", family = "binomial", trControl = log_control)

log_model

```
10-fold cross validation was performed when constructing the model. The training data was partitioned into 10 sets. In each set the data was further divided to a train:validation dataset in a 9:1 ratio. For each set, the model was trained using the train data and tested on the validation data. The accuracy of each model was recorded, and the final accuracy shown here is the average of all accuracies. This gives an estimate of the actual accuracy that will be observed when the model is used on real data.

```{r}
# Collect results of model's coefficients and exponentiated coefficients in a dataframe.
coeffs <- rbind(
  log_model$finalModel$coefficients,
  exp(log_model$finalModel$coefficients)
  ) %>% 
  round(.,4) %>%
  cbind(value = c('coefficient', 'exponentiated_coefficient')) %>%
  as.data.frame()

coeffs
```
The coefficients estimated for each variable indicates the effect a 1 unit change has on the log odds ratio of the outcome we are trying to predict for, while holding all other variables constant. For example, for every 1 year increase in age, the log odds of being diabetic increases by 0.023. This value is more easily interpreted if we exponentiate them. This transforms the log odds to odds ratio, meaning that for every 1 year increase in age, the odds of becoming diabetic increases by 2.4%. 

```{r}
# Use model on test data.
log_predictions <- predict(log_model, 
        newdata = test_df)

log_results <- confusionMatrix(log_predictions, test_df$diabetes, positive = '1')

```

### Construction of decision tree model

The decision tree will attempt to classify diabetics based on the 5 chosen variables. To determine the variable used to split the dataset, the Gini impurity, (a value which determines how good the split is, based on the amount of unique classes left in the child nodes) is calculated for each predictor variable. The lower the Gini impurity, the better the split. The data is then divided into the child nodes, and for each child node, the Gini impurity is calculated using the remaining variables, a variable is used for a split, and the dataset is divided until the terminal nodes have only one class remaining. 

10-fold cross validation was used to optimize the complexity of the tree (cp). Smaller cp values give more complex trees with more branches. But could lead to overfitting. The cp value with the highest accuracy score was used to train the final model.

```{r}

# Optimize cp value for decision trees.
control <- trainControl(method = "repeatedcv", 
                        number = 10,
                        search = "grid")

decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)

decision_tree

# Visualize the optimized model.
rpart.plot(decision_tree$finalModel)

# Use the model to predict diabetic labels.
decision_tree_predictions <- predict(decision_tree, newdata = test_df)

tree_results <- confusionMatrix(decision_tree_predictions, 
                                test_df$diabetes, 
                                positive = '1')

```
The initial tree only has the root node and one split. The model thinks the best variable for classifying the data is the tx variable. The current interpretation of this tree is: if tx = 0 is true, it means the patient is non-diabetic, while if tx = 0 is false, the patient is diabetic. 

The tree is likely being pruned early to prevent overfitting (The model performs well on training data but performs poorly on test data). This happens when a more complicated tree is built, and only a small proportion of the dataset falls within one of the leaf nodes. This gives us poor confidence in the model when doing classification on future datasets, and so the model has decided to use tx as the first and only split to determine if someone was diabetic.

It is possible to force the model to use more variables as splits by adjusting the cp (complexity) argument of rpart(), so this was performed as well. 

```{r}
# Create and visualize a more complex tree. Use rpart to manually specify cp value
decision_tree_low_cp <- rpart(diabetes ~ ., data = train_df, cp = 0.0036)

rpart.plot(decision_tree_low_cp)
```

The second tree made with a lower cp value of 0.0036 forced the model to use age, which was deemed as the 2nd best variable to use as a split. This was not done by the original model as the leaf node from the initial split using tx was already pure, and one of the leaf nodes from the age split contained only 1% of the dataset. There was no point of doing this as overfitting would likely occur and only one split was used for this decision tree.

### Construction of random forest model

The random forest model utilizes many decision trees and bootstrapping. Bootstrapping creates new datasets by selecting random rows from the original dataset, with replacement. For each bootstrapped dataset, a decision tree is constructed. The decision tree's splits are made using the same methodology as the decision tree model, but only a limited subset of all variables are considered at each split. Multiple trees are constructed by repeating the process, and the test data set is passed through each tree before consensus voting occurs. The number of trees voting for each classification (diabetic / non-diabetic) are summed up and the classification with the higher vote count is used. 

A 10-fold cross-validation to optimize the number of variables randomly selected at each split (mtry). For every partition of the data, several values of mtry are tested, and the accuracy is logged. It continues doing this for all partition and averages out the accuracy values for each mtry value before picking the mtry that yields the highest accuracy. The final model is then trained using this mtry value.

```{r}
# Create random forest model.
rf <- train(diabetes ~ ., 
            data = train_df,
            method = 'rf',
            trControl = control)

rf

# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', 
      round(mean(rf$finalModel$err.rate[,1]), 2)*100, 
      '%', 
      sep = ' ')

# Use the model to make predictions on diabetic labels.
rf_predictions <- predict(rf, test_df)

rf_predictions <- as.factor(rf_predictions)

rf_results <- confusionMatrix(rf_predictions, test_df$diabetes, positive = '1')
```
An mtry value of 2 was used as it yielded the highest average accuracy during the cross-validation. We see that the OOB (out of bag) estimate of error is 7%, which means accuracy should be close to 93%. This value refers to the percent of errors encountered when samples that were not used to construct a particular tree were passed through the tree to check for its classification. It also gives an idea of what the accuracy would look like if the model was used on new data. 7% of the out of bag samples were classified incorrectly. 


# **Results**

```{r}
# Result compilation.

# Bind all results into a dataframe.
combined_results <- rbind(
    'Logistic regression' = unlist(log_results),
    'Decision tree' = unlist(tree_results),
    'Random forest' = unlist(rf_results)
  ) %>%
  as.data.frame() %>%
  
  # Remove all column prefixes.
  # ^[^.]+ From the start of the string, detect any number of characters that is not a full stop.
  # \\. Match a fullstop (\\ is used to escape the fullstop).
  rename_all( ~ gsub("^[^.]+\\.", "", .)) %>%
  
  # Select relevant columns.
  select(Accuracy,
         Sensitivity,
         Specificity,
         Precision,
         `Neg Pred Value`,
         F1) %>%
  
  # Convert values from character to float (double), round to 4 digits after decimal points.
  mutate_all(as.double) %>%
  round(., 4) %>%
  gt()

combined_results

```

All models performed similarly to one another based on the 6 metrics. They had accuracies of >92%. This metric is easy to understand, the models had performed the classification correctly in 92% of the individuals. There is large difference between the sensitivity (recall) and specificity metrics. All models had a sensitivity of ~65% and a specificity of ~96%. This means the models are all good at identifying true negative cases, but fare poorly when identifying true positive cases. 

This is not unexpected as raising one would cause the other to decrease, if sensitivity were to increase, thereby improving the model's ability to detect for true positive cases, the number of false positives would increase and specificity would decrease. A better explanation would be that the threshold for classifying something as a positive is reduced to capture all positives, but in doing so people who are truly negative will also get classified as positive, leading to more false positives. In the current scenario, the threshold for a positive classification is very high, so while the model is better at identifying true negatives, there is a higher chance of classifying truly positive people as negative (false negative).

Precision and negative predictive value (false discovery rate) have a similar relationship to that of sensitivity and specificity, which is probably due to the imbalanced dataset as well.

I believe this is due to the dataset being imbalanced, as only 9.24% of the dataset is diabetic, while the rest are not diabetic. This could have allowed the models to learn the characteristics of non-diabetics more accurately than those of diabetics, leading to high specificity. 

Overall I would say the random forest model is the best in this case. It has the highest accuracy and precision scores and the second highest F1 score. I suspect the random forest would also perform better on new data as the model uses a number of randomly constructed decision trees as opposed to the use of a single decision tree, which should allow for more robust predictions.

One possible improvement is to form a new dataset using equal numbers of diabetic and non-diabetic individuals. This can be done through random sampling of the non-diabetic subset until it matches the total number of diabetic patients, the weights of non-diabetic individuals can then be raised by a factor of 10 (because diabetics make up about 10% of the dataset) to account for the differences in number of individuals. Of course, this would also mean the overall dataset becomes a lot smaller. 

In addition, further optimization could be performed by adjusting hyperparameters for all models, in the decision tree and random forests, I used k-fold cross validation to optimize the cp and mtry values. There are many more hyperparameters that can be adjusted but I am not confident in my knowledge in them. It is possible to produce better models by trying different combinations of hyperparameters, but tinkering with something that I don't fully understand is unwise, hence I did not do this. 

