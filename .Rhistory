theme(plot.title = element_text(hjust = 0.5))
print(p)
}
# Prepare dataframe for plotting venn diagram.
pDM_or_DM_venn <- categorical_df %>%
select(dx, gh) %>%
# Filter for row sums >= 1 to exclude individuals who are not in either category. (Not diabetic, not preDM)
filter(rowSums(.) >= 1) %>%
# Transform into T/F for venn diagram plotting.
mutate(dx = as.logical(dx),
gh = as.logical(gh)) %>%
rename('preDM or DM' = dx,
'DM' = gh)
# Plot the venn diagram.
ggvenn(pDM_or_DM_venn, c('preDM or DM', 'DM'))
# The individual's diabetes status is more clearly illustrated below.
# dx = 0, gh = 0 -> no preDM or DM
# dx = 1, gh = 1 -> DM
# dx = 1, gh = 0 -> preDM
# dx = 0, gh = 1 -> DM
# Load data.
query <- 'SELECT age, wt, ht, bmi, leg, arml, armc, waist, tri, sub, gh, albumin, bun, SCr FROM diabetes_data'
continuous_df <- dbGetQuery(conn, query)
# Construct paired geom_point graphs with different features.
continuous_df <- continuous_df %>%
na.omit() %>%
mutate(diabetes = case_when(gh >= 6.5 ~ 1,
T ~ 0))
pair_graphs <- ggpairs(continuous_df, aes(col = diabetes),
progress = F,
lower = list(continuous = "points"),
upper = list(continuous = "blank"),
diag = list(continuous = "blankDiag"),
axisLabels = 'none')
# pair_graphs
# Produce a correlation matrix from a dataframe (pearson's correlation coefficient)
cont_heatmap <- cor(continuous_df)
# Plot the heatmap
pheatmap(cont_heatmap,
display_numbers = T)
# Plot all continous variables against gh.
for (i in colnames(continuous_df)) {
p <- ggplot(continuous_df) +
geom_point(aes_string(x = i, y = 'gh', col = 'diabetes')) +
theme_classic()
print(p)
}
# Initialize a new list for result collection.
t_test_df <- list()
# For all continuous variables except the diabetes columns, do a t-test of means between diabetics and non-diabetics.
for (i in colnames(continuous_df)[1:length(colnames(continuous_df)) - 1]) {
# Perform the t-test.
t_res <- t.test(continuous_df[continuous_df$diabetes == 1, colnames(continuous_df) == i],
continuous_df[continuous_df$diabetes == 0, colnames(continuous_df) == i])
# Collect results into a list.
entry <- list('col_name' = i,
'p_value' = t_res$p.value,
'diabetic_mean' = round(t_res$estimate[[1]],2),
'non_diabetic_mean' = round(t_res$estimate[[2]], 2))
# Add the entry into a list.
t_test_df[[i]] <- entry
}
# R-bind all lists to form a dataframe.
t_test_df <- rbindlist(t_test_df) %>%
arrange(p_value) %>%
mutate(significant = case_when(p_value < 0.05 ~ T,
T ~ F))
t_test_df
# Obtain categorical variables and gh.
query <- 'SELECT sex, re, income, tx, dx, gh FROM diabetes_data'
categorical_df <- dbGetQuery(conn, query) %>%
mutate(diabetes = case_when(gh >= 6.5 ~ T,
T ~ F)) %>%
select(-gh)
# Initialize empty list.
chi_sq_df <- list()
# For all columns except the diabetes marker, perform chi square test.
# Determine if variable in question is significantly associated with diabetes.
for (i in colnames(categorical_df)[1:length(colnames(categorical_df)) - 1]) {
# Construct contingency table, perform chi square test.
cont_table <- table(categorical_df[[i]], categorical_df$diabetes)
chi_squared_result <- chisq.test(cont_table)
cramers_v_result <- assocstats(cont_table)
# Store result in a list.
entry <- list('col_name' = i,
'chi-sq_stat' = chi_squared_result$statistic,
'p_value' = chi_squared_result$p.value,
'cramers_v' = cramers_v_result$cramer)
chi_sq_df[[i]] <- entry
}
# R-bind all lists to form a dataframe.
chi_sq_df <- rbindlist(chi_sq_df) %>%
arrange(p_value) %>%
mutate(significant = case_when(p_value < 0.05 ~ T,
T ~ F))
chi_sq_df
# Load data.
query <- 'SELECT * FROM diabetes_data'
df <- dbGetQuery(conn, query)
# Perform KNN imputation.
after_impute_df <- df %>%
kNN(variable = 'albumin',
k = 5) %>%
rename('albumin_imputed' = albumin_imp)
# Plot new distribution.
alb_after_plot <- ggplot(after_impute_df) +
geom_point(aes(x = albumin, y = gh, col = albumin_imputed))
alb_after_plot
print(paste('albumin NA values before imputation:', sum(is.na(df$albumin))))
print(paste('albumin NA values after imputation:', sum(is.na(after_impute_df$albumin))))
summary(df$albumin)
summary(after_impute_df$albumin)
after_impute_df <- after_impute_df %>%
select(age, bmi, albumin, tx, dx, gh) %>%
mutate(diabetes = case_when(gh >= 6.5 ~ 1,
T ~ 0),
diabetes = as.factor(diabetes)) %>%
select(-gh)
# Split dataset in a 70:30 train:test ratio.
split <- sample.split(after_impute_df, SplitRatio = 0.7)
train_df <- subset(after_impute_df, split == T)
test_df <- subset(after_impute_df, split == F)
# Build a logistic regression model.
log_model <- glm(diabetes ~ .,
data = train_df,
family = 'binomial')
summary(log_model)
exp(coef(log_model))
# Use model on test data.
log_predictions <- predict(log_model,
newdata = test_df,
type = 'response')
# Transform such that values >0.5 = T, else = F.
log_predictions <- ifelse(log_predictions > 0.5, 1, 0)
log_predictions <- as.factor(log_predictions)
log_results <- confusionMatrix(log_predictions, test_df$diabetes, positive = '1')
# Perform k-fold cross validation.
tr_ctrl <- trainControl(method = 'cv', number = 5)
cv_model <- train(diabetes ~ ., data = train_df, method = "glm", family = "binomial", trControl = tr_ctrl)
cv_model
# Optimize vp value for decision trees.
control <- trainControl(method = "repeatedcv",
number = 10, # number of folds
search = "grid")
decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)
decision_tree
# Visualize the optimized model.
rpart.plot(decision_tree$finalModel)
# Make a decision tree classifier using rpart. Needed for comparison later.
decision_tree <- rpart(diabetes~., data = train_df, method = "class",
cp = decision_tree_optimization$bestTune[[1]])
# Optimize vp value for decision trees.
control <- trainControl(method = "repeatedcv",
number = 10, # number of folds
search = "grid")
decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)
decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)
decision_tree
# Visualize the optimized model.
rpart.plot(decision_tree$finalModel)
# Use the model to predict diabetic labels.
decision_tree_predictions <- predict(decision_tree, newdata = test_df, type = 'class')
decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)
decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)
decision_tree
# Use the model to predict diabetic labels.
decision_tree_predictions <- predict(decision_tree, newdata = test_df, type = 'class')
# Use the model to predict diabetic labels.
decision_tree_predictions <- predict(decision_tree, newdata = test_df)
tree_results <- confusionMatrix(decision_tree_predictions,
test_df$diabetes,
positive = '1')
tree_results$byClass
# Optimize vp value for decision trees.
control <- trainControl(method = "repeatedcv",
number = 10, # number of folds
search = "grid")
decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)
decision_tree
# Visualize the optimized model.
rpart.plot(decision_tree$finalModel)
# Use the model to predict diabetic labels.
decision_tree_predictions <- predict(decision_tree, newdata = test_df)
tree_results <- confusionMatrix(decision_tree_predictions,
test_df$diabetes,
positive = '1')
tree_results$byClass
# Create random forest model.
# rf <- randomForest(diabetes~., data = train_df)
rf <- train(diabetes~.,
data = train_df,
method = 'rf',
trControl = control)
rf
# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', round(mean((rf$err.rate)[,1])*100, 2))
# Use the model to make predictions on diabetic labels.
rf_predictions <- predict(rf, test_df)
rf_predictions <- as.factor(rf_predictions)
rf_results <- confusionMatrix(rf_predictions, test_df$diabetes, positive = '1')
log_results
tree_results
rf_results
rf
View(rf)
# Create random forest model.
rf <- randomForest(diabetes~., data = train_df)
rf
View(rf)
rf <- train(diabetes~.,
data = train_df,
method = 'rf',
trControl = control)
rf
rf
View(rf)
rf$finalmodel[1]
rf$finalmodel[[2]]
rf$finalmodel
rf <- train(diabetes~.,
data = train_df,
method = 'rf',
trControl = control)
rf <- train(diabetes~.,
data = train_df,
method = 'rf',
trControl = control)
rf$finalModel
rf
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = 'C:/Users/Dramor/OneDrive - Nanyang Technological University/MIMIC-IV/mimic-iv-clinical-database-demo-2.2/icu')
# Define a relative path to a file or directory
relative_path <- file.path('Data', 'nhgh.tsv')
image_path <- file.path('Images','data_dictionary.png')
# Load packages
library(tidyverse)
library(ggplot2)
library(DBI)
library(naniar)
library(GGally)
library(reshape2)
library(data.table)
library(pheatmap)
library(vcd)
library(ggvenn)
library(VIM)
library(gridExtra)
library(caTools)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
# Setting global seed for reproducibility
set.seed(777)
# Loading of data.
df <- read_tsv(relative_path,
show_col_types = FALSE)
# Open a SQLite connection.
conn <- dbConnect(RSQLite::SQLite(), dbname = 'diabetes.db')
# dbRemoveTable(conn,'diabetes_data')
# dbRemoveTable(conn,'diabetes_data_modified')
# Create a new table within the database.
dbWriteTable(conn,
name = 'diabetes_data',
value = df,
overwrite = TRUE)
# Analyze the whole dataset first.
query <- 'SELECT * FROM diabetes_data'
df <- dbGetQuery(conn, query)
na_plot <- vis_miss(df)
print(na_plot)
aggr(df)
# Count the number of entries.
query <- "SELECT COUNT(*) FROM diabetes_data"
initial_row_count <- dbGetQuery(conn, query)
print(paste('Initial row count:', initial_row_count))
# Count number of entries with atleast one NA in them.
query <- "SELECT COUNT(*) FROM diabetes_data WHERE income iS NULL OR leg IS NULL OR arml IS NULL OR armc IS NULL OR waist IS NULL OR tri IS NULL OR sub IS NULL OR albumin IS NULL OR bun IS NULL OR Scr IS NULL"
row_count_no_NA <- dbGetQuery(conn, query)
print(paste('Rows with atleast one NA:', row_count_no_NA))
# Calculate the amount of data lost.
print(paste('Percent data lost: ', round(row_count_no_NA/initial_row_count*100), '%', sep = ''))
# Obtain continuous variables.
query <- 'SELECT age, wt, ht, bmi, leg, arml, armc, waist, tri, sub, gh, albumin, bun, SCr FROM diabetes_data'
continuous_df <- dbGetQuery(conn, query)
# Transform into a two col dataframe of variables and values. This is necessary for facet_wrap().
continuous_df_long <- melt(continuous_df)
# For each variable, plot a violin+boxplot.
continuous_plot <- ggplot(continuous_df_long, aes(x = variable, y = value)) +
geom_violin(width = 1) +
geom_boxplot(width = 0.5) +
facet_wrap(~ variable, scales = "free") +
theme_classic()
continuous_plot
summary(continuous_df)
# Obtain categorical variables and gh.
query <- 'SELECT sex, re, income, tx, dx, gh FROM diabetes_data'
categorical_df <- dbGetQuery(conn, query)
# Transform gh to categorical variable.
categorical_df <- categorical_df %>%
mutate(gh = case_when(gh >= 6.5 ~ 1,
T ~ c(0)))
# Create bar plots
for (i in colnames(categorical_df)) {
if (i == 'income') {
p <- ggplot(categorical_df) +
geom_bar(aes_string(x = i)) +
theme_classic() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(p)
}
else {
p <- ggplot(categorical_df) +
geom_bar(aes_string(x = i)) +
theme_classic()
print(p)
}
}
# Transform into long format, for all categories, count all values observed.
altered_pie_df <- categorical_df %>%
gather(key = 'category') %>%
count(category, value) %>%
mutate(perc_n = round(n/nrow(categorical_df)*100, 2))
# Create pie charts for every categorical variable.
for (i in unique(altered_pie_df$category)) {
p <- altered_pie_df[altered_pie_df$category == i, ] %>%
ggplot(aes(x="", y=n, fill=value)) +
geom_col(color = 'black') +
geom_text(aes(label = paste(perc_n, '%')),
position = position_stack(vjust = 0.5)) +
coord_polar("y", start=0) +
theme_void() +
ggtitle(paste(i)) +
theme(plot.title = element_text(hjust = 0.5))
print(p)
}
# Prepare dataframe for plotting venn diagram.
pDM_or_DM_venn <- categorical_df %>%
select(dx, gh) %>%
# Filter for row sums >= 1 to exclude individuals who are not in either category. (Not diabetic, not preDM)
filter(rowSums(.) >= 1) %>%
# Transform into T/F for venn diagram plotting.
mutate(dx = as.logical(dx),
gh = as.logical(gh)) %>%
rename('preDM or DM' = dx,
'DM' = gh)
# Plot the venn diagram.
ggvenn(pDM_or_DM_venn, c('preDM or DM', 'DM'))
# The individual's diabetes status is more clearly illustrated below.
# dx = 0, gh = 0 -> no preDM or DM
# dx = 1, gh = 1 -> DM
# dx = 1, gh = 0 -> preDM
# dx = 0, gh = 1 -> DM
# Load data.
query <- 'SELECT age, wt, ht, bmi, leg, arml, armc, waist, tri, sub, gh, albumin, bun, SCr FROM diabetes_data'
continuous_df <- dbGetQuery(conn, query)
# Construct paired geom_point graphs with different features.
continuous_df <- continuous_df %>%
na.omit() %>%
mutate(diabetes = case_when(gh >= 6.5 ~ 1,
T ~ 0))
pair_graphs <- ggpairs(continuous_df, aes(col = diabetes),
progress = F,
lower = list(continuous = "points"),
upper = list(continuous = "blank"),
diag = list(continuous = "blankDiag"),
axisLabels = 'none')
# pair_graphs
# Produce a correlation matrix from a dataframe (pearson's correlation coefficient)
cont_heatmap <- cor(continuous_df)
# Plot the heatmap
pheatmap(cont_heatmap,
display_numbers = T)
# Plot all continous variables against gh.
for (i in colnames(continuous_df)) {
p <- ggplot(continuous_df) +
geom_point(aes_string(x = i, y = 'gh', col = 'diabetes')) +
theme_classic()
print(p)
}
# Initialize a new list for result collection.
t_test_df <- list()
# For all continuous variables except the diabetes columns, do a t-test of means between diabetics and non-diabetics.
for (i in colnames(continuous_df)[1:length(colnames(continuous_df)) - 1]) {
# Perform the t-test.
t_res <- t.test(continuous_df[continuous_df$diabetes == 1, colnames(continuous_df) == i],
continuous_df[continuous_df$diabetes == 0, colnames(continuous_df) == i])
# Collect results into a list.
entry <- list('col_name' = i,
'p_value' = t_res$p.value,
'diabetic_mean' = round(t_res$estimate[[1]],2),
'non_diabetic_mean' = round(t_res$estimate[[2]], 2))
# Add the entry into a list.
t_test_df[[i]] <- entry
}
# R-bind all lists to form a dataframe.
t_test_df <- rbindlist(t_test_df) %>%
arrange(p_value) %>%
mutate(significant = case_when(p_value < 0.05 ~ T,
T ~ F))
t_test_df
# Obtain categorical variables and gh.
query <- 'SELECT sex, re, income, tx, dx, gh FROM diabetes_data'
categorical_df <- dbGetQuery(conn, query) %>%
mutate(diabetes = case_when(gh >= 6.5 ~ T,
T ~ F)) %>%
select(-gh)
# Initialize empty list.
chi_sq_df <- list()
# For all columns except the diabetes marker, perform chi square test.
# Determine if variable in question is significantly associated with diabetes.
for (i in colnames(categorical_df)[1:length(colnames(categorical_df)) - 1]) {
# Construct contingency table, perform chi square test.
cont_table <- table(categorical_df[[i]], categorical_df$diabetes)
chi_squared_result <- chisq.test(cont_table)
cramers_v_result <- assocstats(cont_table)
# Store result in a list.
entry <- list('col_name' = i,
'chi-sq_stat' = chi_squared_result$statistic,
'p_value' = chi_squared_result$p.value,
'cramers_v' = cramers_v_result$cramer)
chi_sq_df[[i]] <- entry
}
# R-bind all lists to form a dataframe.
chi_sq_df <- rbindlist(chi_sq_df) %>%
arrange(p_value) %>%
mutate(significant = case_when(p_value < 0.05 ~ T,
T ~ F))
chi_sq_df
# Load data.
query <- 'SELECT * FROM diabetes_data'
df <- dbGetQuery(conn, query)
# Perform KNN imputation.
after_impute_df <- df %>%
kNN(variable = 'albumin',
k = 5) %>%
rename('albumin_imputed' = albumin_imp)
# Plot new distribution.
alb_after_plot <- ggplot(after_impute_df) +
geom_point(aes(x = albumin, y = gh, col = albumin_imputed))
alb_after_plot
print(paste('albumin NA values before imputation:', sum(is.na(df$albumin))))
print(paste('albumin NA values after imputation:', sum(is.na(after_impute_df$albumin))))
summary(df$albumin)
summary(after_impute_df$albumin)
after_impute_df <- after_impute_df %>%
select(age, bmi, albumin, tx, dx, gh) %>%
mutate(diabetes = case_when(gh >= 6.5 ~ 1,
T ~ 0),
diabetes = as.factor(diabetes)) %>%
select(-gh)
# Split dataset in a 70:30 train:test ratio.
split <- sample.split(after_impute_df, SplitRatio = 0.7)
train_df <- subset(after_impute_df, split == T)
test_df <- subset(after_impute_df, split == F)
# Build a logistic regression model.
log_model <- glm(diabetes ~ .,
data = train_df,
family = 'binomial')
summary(log_model)
exp(coef(log_model))
# Use model on test data.
log_predictions <- predict(log_model,
newdata = test_df,
type = 'response')
# Transform such that values >0.5 = T, else = F.
log_predictions <- ifelse(log_predictions > 0.5, 1, 0)
log_predictions <- as.factor(log_predictions)
log_results <- confusionMatrix(log_predictions, test_df$diabetes, positive = '1')
# Perform k-fold cross validation.
tr_ctrl <- trainControl(method = 'cv', number = 5)
cv_model <- train(diabetes ~ ., data = train_df, method = "glm", family = "binomial", trControl = tr_ctrl)
cv_model
# Optimize vp value for decision trees.
control <- trainControl(method = "repeatedcv",
number = 10, # number of folds
search = "grid")
decision_tree <- train(diabetes ~ ., data = train_df, 'rpart', trControl = control)
decision_tree
# Visualize the optimized model.
rpart.plot(decision_tree$finalModel)
# Use the model to predict diabetic labels.
decision_tree_predictions <- predict(decision_tree, newdata = test_df)
tree_results <- confusionMatrix(decision_tree_predictions,
test_df$diabetes,
positive = '1')
tree_results$byClass
# Create random forest model.
# rf <- randomForest(diabetes~., data = train_df)
rf <- train(diabetes~.,
data = train_df,
method = 'rf',
trControl = control)
rf
rf$finalModel
# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', round(mean((rf$finalModel)[,1])*100, 2))
rf$finalModel
rf$finalModel[[1]]
rf$finalModel$err.rate[,1]
# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', round(mean((rf$finalModel$err.rate[,1])*100, 2))
finalModel
# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', round(mean((rf$finalModel$err.rate[,1])*100, 2))
# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', round(mean((rf$finalModel$err.rate[,1])*100, 2)))
rf$finalModel$err.rate[,1]
# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', mean(rf$finalModel$err.rate[,1]))
# Take the mean of all error rates, multiply by 100.
paste('Out of bag estimate of error rate:', round(mean(rf$finalModel$err.rate[,1]), 2))
